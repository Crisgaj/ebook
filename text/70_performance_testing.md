## Performance Testing

What, exactly, is performance testing?  Before we can define "performance testing", we first need to define "performance".  I assume that at this point in the book, everybody understands what "testing" means.

Most technically savvy people will have a gut feeling for what performance means - systems that are fast, don't take up muh memory, etc.  The more you try to define it, though, the more slippery the definition becomes.  More fast, compared to what?  What if a system uses lots of RAM, and a competing system uses less RAM but more hard drive space?  If System A returns an answer in three seconds, and System B returns an answer in five seconds, which is more performant?  An easy question to answer, unless the questions that they are answering are themselves different.  Performance is one of those concepts that is hard to pin down, because it will mean different things for different systems.  

Imagine a video game where you play a software tester fighting evil Bugs.  You can use the arrow keys to move and the space key to shoot your insecticide gun.  Whenever you press the space key, you will expect your avatar on-screen to shoot the insecticide within, say, 200 milliseconds - anything more and you'll feel like the video game is "lagging" behind your keypresses.  Now imagine a second system, a weather forecasting supercomputing cluster, with the ability to run millions of calculations in parallel.  However, as weather forecasting requires a large number of calculations, it takes half an hour after hitting the "run" command for the system to return what the weather forecast is for tomorrow.  Until that time, the screen just says "Calculating..."  The supercomputer has several orders of magnitude longer response time than the video game, but does this mean the supercomputer is less performant than your video game console?  On the particular __performance indicator__ of response time, one could argue yes, but that performance indicator was not considered very important to the developers of the weather forecasting system.

How do you determine which system has better performance?  The short answer is, you can't - the systems are trying to do different, basically incomparable actions.  How you measure performance will be based upon the kind of system you are testing.  That does not mean, however, that there are no rules or heuristics.  In this chapter, we'll go over some different kinds of performance and how one would go about testing them.  It is not an exhaustive list, though!  The system you are testing may have unique performance aspects which no book of finite length (which this book most assuredly is) could reasonably contain.

### Categories of Performance Indicators

As mentioned above, defining performance for a particular system means determining which performance indicators the user or customer cares about.  Although there are a large number of possible performance indicators, there are two main categories: service-oriented and efficiency-oriented.

A __service-oriented indicator__ measures how well a system is providing a service to a particular user.  These are measured from the user's perspective and quantify aspects of the system the user would directly care about.  Some examples of service-oriented indicators would be average page load time, response time, or what percentage of the time the system is available.

There are two main subcategories of service-oriented indicators: availability, or how available is the system to the user.  This could mean anything from what percentage of the time can the system be accessed to how available different features are to the users at different times.  The second main category is response time - how long does it take for the system to respond to the user's input or return a result?

An __efficiency-oriented indicator__ measures how efficiently a system makes use of available computational resources.  You can think of these as being measured "from the program's point of view", or at least from the computer's point of view.  Examples of efficiency-oriented indicators would be what percentage of CPU operations are being used for performing some functionality, how much disk space is being used, or how many events can concurrent users can the system handle on a particular server?  Although the last may seem like it may be a service-oriented indicator, the fact that it's from the system level - how many users can the system handle, rather than how does it appear to a specific user - means that it's an effiency-oriented indicator.  Whenever you are measuring from the perspective of the system, as opposed to something _directly_ experienced by the user, you are testing an effiency-oriented indicator.

Just like service-oriented indicators, there are two main subcategories in effiency-oriented testing.  __Throughput__ measures how many events can the system handle in a given amount of time, such as how many users can log in at the same time, or how many packets can a system route in 30 seconds.  __Utilization__ measures what percentage or absolute amount of computing resources are used to perform a given task.  For example, how much disk space does a database take to store a specified table?  How much RAM is necessary when sorting it?

Service-oriented indicators are similar to black-box testing in that they measure the system from an external perspective, without determining the cause of the problem precisely.  They are blunt instruments from broadly effective in finding where problems should be looked for more closely.  Efficiency-oriented indicators are similar to white-box testing in that they require knowledge of the system, as well as general technical knowledge.  Oftentimes, you can use service-oriented indicators to determine general areas where performance might be an issue to users, and then use efficiency-oriented indicators to nail down exactly what is causing the problem.  Long response times from the perspective of the user may result from physical RAM being used up and much of the information needing to be swapped in from disk.  Service-oriented testing would catch the former, but efficiency-oriented testing would be able to figure out the latter.

### Testing Performance - Thresholds and Targets

In order to determine whether or not a performance test has "passed", you need __performance targets__, or specific quantitative values that the performance indicators are supposed to reach.  For example, you may have an efficiency-oriented performance target that the installer for the software under test should be less than ten megabytes (it seems like a "Hello, world" program takes up over ten megabytes nowadays, but let's leave that complaint to the side for now).  You may have a service-oriented indicator that the system should respond within 500 milliseconds under normal load.  By quantifying a target, you can write a test and determine whether or not the system has met the target.

Targets, though, are the ideal.  Oftentimes, a particular performance indicator for a system may not reach its target, or may not be able to.  __Performance thresholds__ indicate the point where a performance indicator reaches an absolutely minimal acceptable performance.  For example, while the target for response time may be 500 milliseconds, systems engineers may have determined that the system would be releasable with 3000 millisecond response time.  It would not be great if it only met that mark, but if it takes any longer, the system would probably not be releasable.

A system whose performance indicators merely meet the threshold should not be considered as "passing" that particular performance metric.  There is still work to do!  You can see that the standard "pass"/"fail" metric often used for functional testing is often not really appropriate for performance metrics.  There are shades of gray, and they are often able to be manipulated.  What kind of hardware are you running the tests on?  What operating system?  What other processes are running?  Which web browser are you using to access the site?  By changing these, the same system may show very different results on a performance test.  

### Key Performance Indicators

Although there are a very large number of potential performance indicators for a system, there will always be a subset of them that you are most interested in.  A video game may require very fast response time and high availability, but very little need for minimizing disk space, CPU, or RAM usage.  A long-term daemon process running in the background will have very different requirements - minimal RAM and CPU usage are very important, but response time is really not important at all.  Only rarely will a user or another process interact with it, and when it does, it will be via signals and not expecting a near real-time response.

By selecting the most important performance indicators - referred to as __key performance indicators__ or __KPIs__ - you can focus on testing the aspects of performance most important to the users.  This will then allow developers to tune the system to meet the needs of the users, instead of wasting time on parts of the system which the users will not tend to care about, or already have a sufficient level of performance that spending more time on improving it would be a waste of time.  Depending on the level of specificity of the requirements, it may be possible to determine 

Selecting key performance indicators should be done before testing begins; you should not test a bunch of indicators, then turn around and note which ones were important.  You should determine which ones are most important ahead of time, when designing the test plan for the system.  If you are following a more lightweight development process, with less significant upfront design, you should at least determine whether or not a particular performance indicator is a KPI before running the appropriate test.  This forces you to keep in mind which aspects of the system's performance are important and which are not before coming up with results.  If you try to pick out the KPIs afterwards, you may be tempted (even if subconsciously) to simple select the indicators that met their targets.  Leave that kind of thinking to advertisers.

### Testing Service Oriented Indicators - Response Time

The easiest way to measure response time, of course, is to simply follow this algorithm:

1. Do something
2. Click "start" button on stopwatch
3. Wait for response
4. When response is seen, click "stop" button on stopwatch
5. Write down how long it took

While this may be the easiest way to measure response time, it is far from the best.  There are a plethora of problems with this approach.  First, it is impossible to measure sub-second times; human response time is simply too variable and not fast enough.  You can't measure anything internal to the system if your only interface to the system is what you see.  It's very time-consuming, thus making it difficult to collect large datasets.  It's subject to human error (ever spend an entire day timing things?  At some point, you're bound to forget to click the "start" button, or accidentally click the "reset" button before you've written down the time).  It's an absolutely fantastic way to destroy tester morale (ever spend an entire day timing things?  At some point, you're bound to remember to go look for another job).  Because of all these issues (and more), performance testing is often done with the aid of tools.

Although testing in general is using more automation, performance testing specifically tends to depend on automated tests.  There can be quite a bit of variation in performance indicators from run to run, due to other variables over which you may have little control.  Examples of these variables include other processes running on a server, how much physical RAM was already being used, garbage collection runs, and virtual machine startup times, among others.  Often the only way to get a truly valid result is running a performance test for numerous iterations and statistically analyzing it (obtaining the mean, median, and maximum response times, for example).  The only way to gain a reasonable number of samples in a reasonable amount of time is to automate the process.

...

#### What Is Time?

Although this may sound like a philosophical question, it actually has very direct ramifications when testing response times.  To a computer, there are actually several different ways of measuring time.  To a performance tester, how you report and measure time will actually

The first kind of time, and probably the easiest to understand, is __real time__.  This is the kind of time measures by the clock on the wall, and is thus also referred to as __wall clock time__ (and you thought technical terms would be difficult to learn).  This is analogous to stopwatch time - how long does it take, from the user's perspective, for a program to do something?  Thus, this is usually the kind of time that users will care most about.  However, it does not tell the whole story.

Real time takes into account _everything_ that needs to be done by the system to perform the given task.  If this means reading data in from the disk, or over the network, that counts towards the real time total - no matter how slow the disk or bandwidth-restricted the network.  If this means that it's running on a system which is running lots of other processes concurrently, meaning that it only got a chance to use 5% of the CPU's time 

